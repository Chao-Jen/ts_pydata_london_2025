{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Bayesian Time Series Models\n",
    "\n",
    "#### PyData London 2025 - Bayesian Time Series Analysis with PyMC\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we'll explore the landscape of Bayesian time series models, from simple random walks to sophisticated state-space models and Gaussian processes. These models provide the foundation for understanding temporal patterns and making probabilistic forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Section 3\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import warnings\n",
    "\n",
    "# Configure plotting and suppress warnings\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RNG = np.random.default_rng(RANDOM_SEED:=42)\n",
    "\n",
    "print(\"🔧 Libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Data prepared: 228 observations\n",
      "   Date range: 1970-1988\n",
      "   Standardized data: mean=-0.000, std=1.000\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data (consistent with previous sections)\n",
    "births_data = pl.read_csv('../data/births.csv', null_values=['null', 'NA', '', 'NULL'])\n",
    "births_data = births_data.filter(pl.col('day').is_not_null())\n",
    "\n",
    "monthly_births = (births_data\n",
    "    .group_by(['year', 'month'])\n",
    "    .agg(pl.col('births').sum())\n",
    "    .sort(['year', 'month'])\n",
    ")\n",
    "\n",
    "births_subset = (monthly_births\n",
    "    .filter((pl.col('year') >= 1970) & (pl.col('year') <= 1990))\n",
    "    .with_row_index('index')\n",
    ")\n",
    "\n",
    "# Standardize the data\n",
    "original_data = births_subset['births'].to_numpy()\n",
    "births_standardized = (original_data - original_data.mean()) / original_data.std()\n",
    "n_obs = len(births_standardized)\n",
    "\n",
    "# Create time variables for regression models\n",
    "time_idx = np.arange(n_obs)\n",
    "time_normalized = (time_idx - time_idx.mean()) / time_idx.std()\n",
    "\n",
    "print(f\"📊 Data prepared: {n_obs} observations\")\n",
    "print(f\"   Date range: {births_subset['year'].min()}-{births_subset['year'].max()}\")\n",
    "print(f\"   Standardized data: mean={births_standardized.mean():.3f}, std={births_standardized.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gaussian Random Walks in PyMC\n",
    "\n",
    "Random walks form the foundation of many time series models. They provide a natural way to model **smooth trends**, **level shifts**, and **time-varying parameters** while maintaining computational tractability and clear interpretation.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "A **Gaussian Random Walk** is defined as:\n",
    "\n",
    "$$x_t = x_{t-1} + \\epsilon_t$$\n",
    "\n",
    "where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ represents the **innovation** or step size.\n",
    "\n",
    "With **drift** (systematic trend):\n",
    "$$x_t = x_{t-1} + \\mu + \\epsilon_t$$\n",
    "\n",
    "### Why Random Walks Excel for Time Series\n",
    "\n",
    "**Flexibility** allows random walks to adapt to various trend patterns without imposing rigid functional forms. **Smoothness** ensures that changes occur gradually rather than abruptly, which matches many real-world processes. **Uncertainty propagation** naturally increases over time, reflecting our decreasing confidence in long-term predictions. **Parameter interpretability** makes the models easy to understand and communicate to stakeholders.\n",
    "\n",
    "### Applications and Use Cases\n",
    "\n",
    "Random walks excel at modeling **economic indicators** that evolve smoothly over time, **demographic trends** that change gradually, **environmental measurements** with natural variation, and **latent states** in more complex hierarchical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Random Walk\n",
    "\n",
    "Let's start with the simplest time series model: a random walk without drift. A **Gaussian Random Walk** is defined mathematically as:\n",
    "\n",
    "$$x_t = x_{t-1} + \\epsilon_t$$\n",
    "\n",
    "where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\text{walk}}^2)$ represents the **innovation** or step size at each time period.\n",
    "\n",
    "The key properties of this process include:\n",
    "\n",
    "- **Non-stationarity**: The variance increases linearly with time as $\\text{Var}(x_t) = t \\cdot \\sigma_{\\text{walk}}^2$\n",
    "- **Martingale property**: $\\mathbb{E}[x_t | x_{t-1}, x_{t-2}, \\ldots] = x_{t-1}$\n",
    "- **Infinite memory**: All past shocks have permanent effects on the current level\n",
    "- **No mean reversion**: The process has no tendency to return to any particular value\n",
    "\n",
    "In our implementation, we separate the **latent random walk process** from the **observation process**, creating a state-space structure:\n",
    "\n",
    "**State equation**: $x_t = x_{t-1} + \\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\text{walk}}^2)$\n",
    "\n",
    "**Observation equation**: $y_t = x_t + \\nu_t$ where $\\nu_t \\sim \\mathcal{N}(0, \\sigma_{\\text{obs}}^2)$\n",
    "\n",
    "This separation allows us to distinguish between **process noise** (how much the underlying state changes) and **measurement noise** (how much error exists in our observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 228 out of bounds for int8\n",
      "\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma_walk, walk, sigma_obs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e280d925774457c812a55512de17e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 **Random Walk Model Results**:\n",
      "             mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "sigma_walk  0.382  0.040   0.308    0.455      0.002    0.001     501.0   \n",
      "sigma_obs   0.329  0.035   0.258    0.389      0.002    0.001     514.0   \n",
      "\n",
      "            ess_tail  r_hat  \n",
      "sigma_walk     825.0    1.0  \n",
      "sigma_obs      674.0    1.0  \n",
      "\n",
      "📈 **Model Interpretation**:\n",
      "   • **Innovation variance**: 0.382 - controls trend smoothness\n",
      "   • **Observation noise**: 0.329 - measurement error level\n",
      "   • **Signal-to-noise ratio**: 1.161\n"
     ]
    }
   ],
   "source": [
    "# Simple Random Walk\n",
    "with pm.Model() as random_walk_model:\n",
    "    # Step size of the random walk (innovation standard deviation)\n",
    "    sigma_walk = pm.HalfNormal('sigma_walk', sigma=1.0)\n",
    "    \n",
    "    # Initial value distribution\n",
    "    init_dist = pm.Normal.dist(mu=0, sigma=1)\n",
    "    \n",
    "    # Gaussian random walk process\n",
    "    walk = pm.GaussianRandomWalk('walk', \n",
    "                                mu=0,  # no drift\n",
    "                                sigma=sigma_walk, \n",
    "                                init_dist=init_dist,\n",
    "                                steps=n_obs-1)\n",
    "    \n",
    "    # Observation noise (measurement error)\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=1.0)\n",
    "    \n",
    "    # Likelihood: observed data are noisy observations of the walk\n",
    "    y_pred = pm.Normal('y_pred', mu=walk, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from the model\n",
    "with random_walk_model:\n",
    "    trace_rw = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Random Walk Model Results**:\")\n",
    "print(az.summary(trace_rw, var_names=['sigma_walk', 'sigma_obs']))\n",
    "\n",
    "# Extract and display key insights\n",
    "sigma_walk_mean = az.extract(trace_rw)['sigma_walk'].mean().item()\n",
    "sigma_obs_mean = az.extract(trace_rw)['sigma_obs'].mean().item()\n",
    "\n",
    "print(f\"\\n📈 **Model Interpretation**:\")\n",
    "print(f\"   • **Innovation variance**: {sigma_walk_mean:.3f} - controls trend smoothness\")\n",
    "print(f\"   • **Observation noise**: {sigma_obs_mean:.3f} - measurement error level\")\n",
    "print(f\"   • **Signal-to-noise ratio**: {sigma_walk_mean/sigma_obs_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk with Drift\n",
    "\n",
    "Now let's add a **drift term** to capture any underlying systematic trend in the data. A **Random Walk with Drift** extends the basic random walk by including a deterministic trend component:\n",
    "\n",
    "$$x_t = x_{t-1} + \\mu + \\epsilon_t$$\n",
    "\n",
    "where $\\mu$ is the **drift parameter** representing the expected change per time period, and $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\text{walk}}^2)$.\n",
    "\n",
    "The mathematical properties of this process include:\n",
    "\n",
    "- **Expected value**: $\\mathbb{E}[x_t] = x_0 + \\mu \\cdot t$ (linear trend)\n",
    "- **Variance**: $\\text{Var}(x_t) = t \\cdot \\sigma_{\\text{walk}}^2$ (increases with time)\n",
    "- **Covariance**: $\\text{Cov}(x_s, x_t) = \\min(s,t) \\cdot \\sigma_{\\text{walk}}^2$ for $s \\leq t$\n",
    "\n",
    "The drift parameter $\\mu$ allows the model to capture **systematic directional movement** while maintaining the **stochastic flexibility** of random walks. Positive drift indicates an upward trend, negative drift indicates a downward trend, and zero drift reduces to the simple random walk.\n",
    "\n",
    "This model is particularly useful for:\n",
    "- **Economic time series** with underlying growth trends\n",
    "- **Population dynamics** with systematic changes\n",
    "- **Financial assets** with risk premiums\n",
    "- **Climate variables** with long-term trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "ERROR (pytensor.graph.rewriting.basic): Rewrite failure due to: local_subtensor_merge\n",
      "ERROR (pytensor.graph.rewriting.basic): node: Subtensor{i}(Subtensor{start:stop}.0, 0)\n",
      "ERROR (pytensor.graph.rewriting.basic): TRACEBACK:\n",
      "ERROR (pytensor.graph.rewriting.basic): Traceback (most recent call last):\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1922, in process_node\n",
      "    replacements = node_rewriter.transform(fgraph, node)\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/graph/rewriting/basic.py\", line 1086, in transform\n",
      "    return self.fn(fgraph, node)\n",
      "           ~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 542, in local_subtensor_merge\n",
      "    merge_two_slices(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        fgraph, slice1, xshape[pos_1], slices2[pos_2], ushape[pos_2]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/home/fonnesbeck/repos/ts_pydata_london_2025/.pixi/envs/default/lib/python3.13/site-packages/pytensor/tensor/rewriting/subtensor.py\", line 1109, in merge_two_slices\n",
      "    n_val = sl1.stop - 1 - sl2 * sl1.step\n",
      "            ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "OverflowError: Python integer 229 out of bounds for int8\n",
      "\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_drift, sigma_walk, rw_drift, sigma_obs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3010c7946c6f4af39fc5caeaa0746f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 **Random Walk with Drift Results**:\n",
      "             mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "mu_drift    0.004  0.025  -0.041    0.052      0.000    0.001    4106.0   \n",
      "sigma_walk  0.386  0.041   0.317    0.464      0.002    0.001     452.0   \n",
      "sigma_obs   0.326  0.036   0.257    0.391      0.002    0.001     406.0   \n",
      "\n",
      "            ess_tail  r_hat  \n",
      "mu_drift      1592.0   1.00  \n",
      "sigma_walk     830.0   1.01  \n",
      "sigma_obs      622.0   1.01  \n",
      "\n",
      "📈 **Drift Analysis**:\n",
      "   • **Estimated drift**: 0.0041 per time period\n",
      "   • **95% HDI**: [-0.0411, 0.0523]\n",
      "   • **Interpretation**: No clear systematic trend detected\n"
     ]
    }
   ],
   "source": [
    "# Random Walk with Drift\n",
    "with pm.Model() as rw_drift_model:\n",
    "    # Drift parameter (systematic trend)\n",
    "    mu_drift = pm.Normal('mu_drift', mu=0, sigma=0.1)\n",
    "    \n",
    "    # Innovation variance (random fluctuations)\n",
    "    sigma_walk = pm.HalfNormal('sigma_walk', sigma=0.5)\n",
    "    \n",
    "    # Random walk with drift\n",
    "    rw_drift = pm.GaussianRandomWalk('rw_drift', \n",
    "                                    mu=mu_drift,  # Drift term\n",
    "                                    sigma=sigma_walk,\n",
    "                                    steps=n_obs-1)\n",
    "    \n",
    "    # Observation equation\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=0.5)\n",
    "    obs = pm.Normal('obs', mu=rw_drift, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from the model\n",
    "with rw_drift_model:\n",
    "    trace_rw_drift = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Random Walk with Drift Results**:\")\n",
    "print(az.summary(trace_rw_drift, var_names=['mu_drift', 'sigma_walk', 'sigma_obs']))\n",
    "\n",
    "# Extract and interpret results\n",
    "mu_drift_mean = az.extract(trace_rw_drift)['mu_drift'].mean().item()\n",
    "mu_drift_hdi = az.hdi(trace_rw_drift, var_names=['mu_drift'])['mu_drift']\n",
    "\n",
    "print(f\"\\n📈 **Drift Analysis**:\")\n",
    "print(f\"   • **Estimated drift**: {mu_drift_mean:.4f} per time period\")\n",
    "print(f\"   • **95% HDI**: [{mu_drift_hdi[0]:.4f}, {mu_drift_hdi[1]:.4f}]\")\n",
    "if mu_drift_hdi[0] > 0:\n",
    "    print(f\"   • **Interpretation**: Strong evidence for positive trend\")\n",
    "elif mu_drift_hdi[1] < 0:\n",
    "    print(f\"   • **Interpretation**: Strong evidence for negative trend\")\n",
    "else:\n",
    "    print(f\"   • **Interpretation**: No clear systematic trend detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Model AR(1)\n",
    "\n",
    "**Autoregressive models** capture temporal dependence by relating current observations to past observations. An **AR(1) model** is defined as:\n",
    "\n",
    "$$y_t = \\phi y_{t-1} + \\epsilon_t$$\n",
    "\n",
    "where $\\phi$ is the **autoregressive coefficient** and $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ is white noise.\n",
    "\n",
    "**Key Mathematical Properties**:\n",
    "\n",
    "- **Stationarity condition**: $|\\phi| < 1$ ensures the process is stationary\n",
    "- **Mean**: $\\mathbb{E}[y_t] = 0$ (assuming zero mean)\n",
    "- **Variance**: $\\text{Var}(y_t) = \\frac{\\sigma^2}{1 - \\phi^2}$ (constant for stationary process)\n",
    "- **Autocorrelation**: $\\rho(k) = \\phi^k$ (exponential decay)\n",
    "\n",
    "**Interpretation of $\\phi$**:\n",
    "- $\\phi > 0$: **Positive persistence** - high values tend to be followed by high values\n",
    "- $\\phi < 0$: **Oscillatory behavior** - high values tend to be followed by low values\n",
    "- $\\phi \\approx 1$: **Near unit root** - very persistent, close to random walk behavior\n",
    "- $\\phi = 0$: **White noise** - no temporal dependence\n",
    "\n",
    "The **half-life** of shocks is approximately $\\ln(0.5)/\\ln(|\\phi|)$ periods, indicating how long it takes for the effect of a shock to decay to half its initial impact.\n",
    "\n",
    "AR models are fundamental building blocks that can be extended to:\n",
    "- **AR(p)**: Higher-order autoregressive models\n",
    "- **ARMA(p,q)**: Combined autoregressive and moving average models\n",
    "- **VAR**: Vector autoregressive models for multiple time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(1) Model\n",
    "with pm.Model() as ar1_model:\n",
    "    # AR coefficient (ensure stationarity)\n",
    "    rho = pm.Beta('rho', alpha=1, beta=1)  # Beta(1,1) = Uniform(0,1)\n",
    "    # Transform to (-1, 1) for AR coefficient\n",
    "    phi = pm.Deterministic('phi', 2 * rho - 1)\n",
    "    \n",
    "    # Innovation variance\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # AR(1) process\n",
    "    ar1 = pm.AR('ar1', rho=phi, sigma=sigma, constant=False, steps=n_obs-1)\n",
    "    \n",
    "    # Likelihood - observed data\n",
    "    obs = pm.Normal('obs', mu=ar1, sigma=0.1, observed=births_standardized[1:])\n",
    "\n",
    "# Sample from AR(1) model\n",
    "with ar1_model:\n",
    "    trace_ar1 = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **AR(1) Model Results**:\")\n",
    "print(az.summary(trace_ar1, var_names=['phi', 'sigma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bayesian Regression for Time Series\n",
    "\n",
    "Bayesian regression models provide an explicit approach to decomposing time series into interpretable components. Unlike random walks that learn patterns implicitly, regression models allow us to specify **linear trends**, **polynomial patterns**, and **seasonal components** directly.\n",
    "\n",
    "### Advantages of Explicit Component Modeling\n",
    "\n",
    "**Interpretability** stands as the primary advantage—each parameter has a clear meaning that can be communicated to stakeholders. **Flexibility** allows us to easily add or remove components based on domain knowledge. **Prior incorporation** becomes natural when we understand what each parameter represents. **Forecasting** benefits from explicit trend and seasonal components that can be extrapolated with confidence.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "The general form of a Bayesian regression time series model is:\n",
    "\n",
    "$$y_t = \\mu + \\beta_1 f_1(t) + \\beta_2 f_2(t) + \\ldots + \\beta_k f_k(t) + \\epsilon_t$$\n",
    "\n",
    "where $f_i(t)$ are **basis functions** that capture different temporal patterns, and $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Trend Model\n",
    "\n",
    "The simplest regression approach models the time series as a **linear function of time** plus noise. The mathematical specification is:\n",
    "\n",
    "$$y_t = \\alpha + \\beta t + \\epsilon_t$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the **intercept** (baseline level at time zero)\n",
    "- $\\beta$ is the **slope** (rate of change per time unit)\n",
    "- $t$ is the **time index** (often centered for numerical stability)\n",
    "- $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ is the **error term**\n",
    "\n",
    "This model assumes:\n",
    "- **Constant rate of change**: The trend is linear over time\n",
    "- **Homoscedastic errors**: Constant variance across time\n",
    "- **Independent errors**: No temporal correlation in residuals\n",
    "\n",
    "The linear trend model works well when the underlying process has a consistent directional trend and is particularly useful for:\n",
    "- **Economic indicators** with steady growth\n",
    "- **Population growth** in stable environments\n",
    "- **Technology adoption** in early phases\n",
    "- **Climate variables** with monotonic changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Trend Model\n",
    "with pm.Model() as linear_trend_model:\n",
    "    # Intercept (baseline level)\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "    \n",
    "    # Trend coefficient (change per time unit)\n",
    "    beta_trend = pm.Normal('beta_trend', mu=0, sigma=1)\n",
    "    \n",
    "    # Expected value (linear trend)\n",
    "    mu_t = pm.Deterministic('mu_t', alpha + beta_trend * time_normalized)\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=mu_t, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from linear trend model\n",
    "with linear_trend_model:\n",
    "    trace_linear = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Trend Model\n",
    "\n",
    "When linear trends are insufficient, **polynomial terms** can capture more complex trend patterns. A **quadratic trend model** extends the linear model:\n",
    "\n",
    "$$y_t = \\alpha + \\beta_1 t + \\beta_2 t^2 + \\epsilon_t$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the **intercept**\n",
    "- $\\beta_1$ is the **linear coefficient** (initial rate of change)\n",
    "- $\\beta_2$ is the **quadratic coefficient** (acceleration/deceleration)\n",
    "- $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ is the error term\n",
    "\n",
    "**Interpretation of coefficients**:\n",
    "- $\\beta_2 > 0$: **Accelerating growth** (upward curvature)\n",
    "- $\\beta_2 < 0$: **Decelerating growth** (downward curvature)\n",
    "- $\\beta_2 = 0$: **Linear trend** (reduces to linear model)\n",
    "\n",
    "The **instantaneous rate of change** at time $t$ is:\n",
    "$$\\frac{dy_t}{dt} = \\beta_1 + 2\\beta_2 t$$\n",
    "\n",
    "**Cautions with polynomial models**:\n",
    "- **Overfitting risk**: Higher-order polynomials can fit noise\n",
    "- **Extrapolation problems**: Polynomials can behave poorly outside the data range\n",
    "- **Multicollinearity**: Powers of time are often highly correlated\n",
    "- **Interpretability**: Higher-order terms become difficult to interpret\n",
    "\n",
    "**Best practices**: Use centered time variables, apply regularizing priors for higher-order terms, and validate extrapolation performance carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Trend Model (quadratic)\n",
    "with pm.Model() as poly_trend_model:\n",
    "    # Intercept\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "    \n",
    "    # Linear trend coefficient\n",
    "    beta_1 = pm.Normal('beta_1', mu=0, sigma=1)\n",
    "    \n",
    "    # Quadratic trend coefficient (use smaller prior variance)\n",
    "    beta_2 = pm.Normal('beta_2', mu=0, sigma=0.5)\n",
    "    \n",
    "    # Expected value (quadratic trend)\n",
    "    mu_t = pm.Deterministic('mu_t', \n",
    "                           alpha + \n",
    "                           beta_1 * time_normalized + \n",
    "                           beta_2 * time_normalized**2)\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=mu_t, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from polynomial trend model\n",
    "with poly_trend_model:\n",
    "    trace_poly = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Components with Fourier Terms\n",
    "\n",
    "**Fourier terms** provide an elegant way to model seasonal patterns using trigonometric functions. For a time series with period $P$, we can represent seasonal patterns as:\n",
    "\n",
    "$$s_t = \\sum_{k=1}^{K} \\left[ \\beta_{s,k} \\sin\\left(\\frac{2\\pi k t}{P}\\right) + \\beta_{c,k} \\cos\\left(\\frac{2\\pi k t}{P}\\right) \\right]$$\n",
    "\n",
    "where:\n",
    "- $P$ is the **seasonal period** (e.g., 12 for monthly data with annual seasonality)\n",
    "- $K$ is the **number of harmonics** (typically $K \\leq P/2$)\n",
    "- $\\beta_{s,k}, \\beta_{c,k}$ are the **Fourier coefficients**\n",
    "\n",
    "**Mathematical properties**:\n",
    "- **Periodicity**: $s_{t+P} = s_t$ for all $t$\n",
    "- **Orthogonality**: Different harmonics are orthogonal over complete periods\n",
    "- **Amplitude**: $A_k = \\sqrt{\\beta_{s,k}^2 + \\beta_{c,k}^2}$ for harmonic $k$\n",
    "- **Phase**: $\\phi_k = \\arctan(\\beta_{s,k}/\\beta_{c,k})$ for harmonic $k$\n",
    "\n",
    "**Advantages of Fourier representation**:\n",
    "- **Flexibility**: Can approximate any periodic function\n",
    "- **Parsimony**: Few parameters can capture complex seasonal patterns\n",
    "- **Interpretability**: Each harmonic represents a specific frequency component\n",
    "- **Computational efficiency**: Fast evaluation and differentiation\n",
    "\n",
    "**Harmonic interpretation**:\n",
    "- **First harmonic** ($k=1$): Fundamental seasonal cycle\n",
    "- **Second harmonic** ($k=2$): Half-period cycles (e.g., bi-annual patterns)\n",
    "- **Higher harmonics**: Capture finer seasonal details and asymmetries\n",
    "\n",
    "The complete model combines trend and seasonality:\n",
    "$$y_t = \\alpha + \\beta t + s_t + \\epsilon_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend + Seasonality with Fourier Terms\n",
    "# Create seasonal components (annual cycle for monthly data)\n",
    "period = 12  # Monthly data with annual seasonality\n",
    "seasonal_freq = 2 * np.pi / period\n",
    "sin_seasonal = np.sin(seasonal_freq * time_idx)\n",
    "cos_seasonal = np.cos(seasonal_freq * time_idx)\n",
    "\n",
    "# Add second harmonic for more flexible seasonality\n",
    "sin_seasonal_2 = np.sin(2 * seasonal_freq * time_idx)\n",
    "cos_seasonal_2 = np.cos(2 * seasonal_freq * time_idx)\n",
    "\n",
    "with pm.Model() as seasonal_model:\n",
    "    # Intercept\n",
    "    mu_overall = pm.Normal('mu_overall', mu=0, sigma=1)\n",
    "    \n",
    "    # Trend coefficient\n",
    "    beta_trend = pm.Normal('beta_trend', mu=0, sigma=1)\n",
    "    \n",
    "    # First harmonic seasonal coefficients\n",
    "    beta_sin_1 = pm.Normal('beta_sin_1', mu=0, sigma=1)\n",
    "    beta_cos_1 = pm.Normal('beta_cos_1', mu=0, sigma=1)\n",
    "    \n",
    "    # Second harmonic seasonal coefficients\n",
    "    beta_sin_2 = pm.Normal('beta_sin_2', mu=0, sigma=0.5)\n",
    "    beta_cos_2 = pm.Normal('beta_cos_2', mu=0, sigma=0.5)\n",
    "    \n",
    "    # Expected value (trend + seasonality)\n",
    "    mu_t = pm.Deterministic('mu_t', \n",
    "                           mu_overall + \n",
    "                           beta_trend * time_normalized + \n",
    "                           beta_sin_1 * sin_seasonal + \n",
    "                           beta_cos_1 * cos_seasonal +\n",
    "                           beta_sin_2 * sin_seasonal_2 + \n",
    "                           beta_cos_2 * cos_seasonal_2)\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=mu_t, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from seasonal model\n",
    "with seasonal_model:\n",
    "    trace_seasonal = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Seasonal Model Results**:\")\n",
    "print(az.summary(trace_seasonal, var_names=['mu_overall', 'beta_trend', 'beta_sin_1', 'beta_cos_1', 'sigma_obs']))\n",
    "\n",
    "# Calculate seasonal amplitude\n",
    "beta_sin_1_mean = az.extract(trace_seasonal)['beta_sin_1'].mean().item()\n",
    "beta_cos_1_mean = az.extract(trace_seasonal)['beta_cos_1'].mean().item()\n",
    "seasonal_amplitude = np.sqrt(beta_sin_1_mean**2 + beta_cos_1_mean**2)\n",
    "\n",
    "print(f\"\\n📈 **Seasonal Analysis**:\")\n",
    "print(f\"   • **Seasonal amplitude**: {seasonal_amplitude:.3f}\")\n",
    "print(f\"   • **Seasonal strength**: {seasonal_amplitude / births_standardized.std():.1%} of total variation\")\n",
    "if seasonal_amplitude > 0.2:\n",
    "    print(f\"   • **Interpretation**: Strong seasonal pattern detected\")\n",
    "else:\n",
    "    print(f\"   • **Interpretation**: Weak seasonal pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: State-Space Models in PyMC\n",
    "\n",
    "State-space models represent one of the most powerful frameworks for time series analysis. They separate the **latent state** (unobserved process) from the **observation process** (what we actually measure), enabling sophisticated modeling of complex temporal dynamics.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "State-space models consist of two equations:\n",
    "\n",
    "**State equation** (evolution): $x_t = f(x_{t-1}, \\theta) + \\eta_t$\n",
    "\n",
    "**Observation equation** (measurement): $y_t = g(x_t, \\phi) + \\epsilon_t$\n",
    "\n",
    "where $x_t$ represents the **latent state**, $y_t$ is the **observation**, $\\eta_t$ and $\\epsilon_t$ are **noise terms**, and $\\theta, \\phi$ are **parameters**.\n",
    "\n",
    "### Advantages of State-Space Modeling\n",
    "\n",
    "**Latent variable modeling** allows us to separate the true underlying process from measurement noise and observation errors. **Missing data handling** becomes natural since we can estimate latent states even when observations are missing. **Hierarchical structure** enables modeling of complex relationships between multiple time series. **Uncertainty propagation** flows naturally through the state evolution, providing realistic prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Level Model (Dynamic Linear Model)\n",
    "\n",
    "The **local level model** is the simplest state-space model, consisting of two equations:\n",
    "\n",
    "**State equation**: $\\mu_t = \\mu_{t-1} + \\eta_t$ where $\\eta_t \\sim \\mathcal{N}(0, \\sigma_{\\eta}^2)$\n",
    "\n",
    "**Observation equation**: $y_t = \\mu_t + \\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2)$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the **latent level** (unobserved state)\n",
    "- $\\eta_t$ is the **state innovation** (level changes)\n",
    "- $\\epsilon_t$ is the **observation noise** (measurement error)\n",
    "- $\\sigma_{\\eta}^2$ controls **state variability**\n",
    "- $\\sigma_{\\epsilon}^2$ controls **observation noise**\n",
    "\n",
    "**Key properties**:\n",
    "- **Signal-to-noise ratio**: $q = \\sigma_{\\eta}^2 / \\sigma_{\\epsilon}^2$ determines smoothness\n",
    "- **High $q$**: Flexible tracking of level changes (noisy estimates)\n",
    "- **Low $q$**: Smooth level estimates (slow adaptation)\n",
    "- **Kalman filtering**: Optimal recursive estimation of latent states\n",
    "\n",
    "**Applications**:\n",
    "- **Trend extraction** from noisy economic indicators\n",
    "- **Signal processing** in engineering applications\n",
    "- **Baseline estimation** for anomaly detection\n",
    "- **Missing data interpolation** using state estimates\n",
    "\n",
    "This model excels at **trend estimation** and **signal extraction** from noisy data while naturally handling missing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Level Model (State-Space)\n",
    "with pm.Model() as local_level_model:\n",
    "    # Process noise (state evolution variance)\n",
    "    sigma_level = pm.HalfNormal('sigma_level', sigma=0.5)\n",
    "    \n",
    "    # Observation noise (measurement error variance)\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=0.5)\n",
    "    \n",
    "    # Initial level\n",
    "    init_level = pm.Normal('init_level', mu=0, sigma=1)\n",
    "    \n",
    "    # Level process (latent state - random walk)\n",
    "    init_dist = pm.Normal.dist(mu=init_level, sigma=sigma_level)\n",
    "    level = pm.GaussianRandomWalk('level',\n",
    "                                 mu=0,  # no drift in level\n",
    "                                 sigma=sigma_level,\n",
    "                                 init_dist=init_dist,\n",
    "                                 steps=n_obs-1)\n",
    "    \n",
    "    # Observations (noisy measurements of latent state)\n",
    "    obs = pm.Normal('obs', mu=level, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from local level model\n",
    "with local_level_model:\n",
    "    trace_local_level = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Local Level Model Results**:\")\n",
    "print(az.summary(trace_local_level, var_names=['sigma_level', 'sigma_obs', 'init_level']))\n",
    "\n",
    "# Analyze signal-to-noise ratio\n",
    "sigma_level_mean = az.extract(trace_local_level)['sigma_level'].mean().item()\n",
    "sigma_obs_mean = az.extract(trace_local_level)['sigma_obs'].mean().item()\n",
    "signal_noise_ratio = sigma_level_mean / sigma_obs_mean\n",
    "\n",
    "print(f\"\\n📈 **State-Space Analysis**:\")\n",
    "print(f\"   • **Process noise**: {sigma_level_mean:.3f} - latent state variability\")\n",
    "print(f\"   • **Observation noise**: {sigma_obs_mean:.3f} - measurement error\")\n",
    "print(f\"   • **Signal-to-noise ratio**: {signal_noise_ratio:.3f}\")\n",
    "if signal_noise_ratio > 1:\n",
    "    print(f\"   • **Interpretation**: High signal relative to noise - clear underlying trend\")\n",
    "else:\n",
    "    print(f\"   • **Interpretation**: High noise relative to signal - noisy measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Linear Trend Model\n",
    "\n",
    "The **local linear trend model** extends the local level model by adding a **time-varying slope** component. This model consists of three equations:\n",
    "\n",
    "**Level equation**: $\\mu_t = \\mu_{t-1} + \\beta_{t-1} + \\eta_{\\mu,t}$ where $\\eta_{\\mu,t} \\sim \\mathcal{N}(0, \\sigma_{\\mu}^2)$\n",
    "\n",
    "**Trend equation**: $\\beta_t = \\beta_{t-1} + \\eta_{\\beta,t}$ where $\\eta_{\\beta,t} \\sim \\mathcal{N}(0, \\sigma_{\\beta}^2)$\n",
    "\n",
    "**Observation equation**: $y_t = \\mu_t + \\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2)$\n",
    "\n",
    "where:\n",
    "- $\\mu_t$ is the **time-varying level**\n",
    "- $\\beta_t$ is the **time-varying slope** (trend)\n",
    "- $\\eta_{\\mu,t}$ is the **level innovation**\n",
    "- $\\eta_{\\beta,t}$ is the **slope innovation**\n",
    "- $\\epsilon_t$ is the **observation noise**\n",
    "\n",
    "**Key features**:\n",
    "- **Adaptive trending**: The slope can change over time\n",
    "- **Smooth evolution**: Both level and slope follow random walks\n",
    "- **Flexible growth**: Can model accelerating, decelerating, or changing direction\n",
    "- **Parameter interpretation**: $\\sigma_{\\beta}^2$ controls how much the growth rate varies\n",
    "\n",
    "**Special cases**:\n",
    "- $\\sigma_{\\beta}^2 = 0$: **Constant slope** (reduces to linear trend)\n",
    "- $\\sigma_{\\mu}^2 = 0$: **Deterministic level** given trend\n",
    "- $\\sigma_{\\beta}^2 \\gg \\sigma_{\\mu}^2$: **Highly variable growth rates**\n",
    "\n",
    "This model allows both the level and the trend to evolve over time, providing greater flexibility for modeling changing growth rates in economic, demographic, or technological time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Linear Trend Model\n",
    "with pm.Model() as local_trend_model:\n",
    "    # Process noise variances\n",
    "    sigma_level = pm.HalfNormal('sigma_level', sigma=0.3)  # level innovation\n",
    "    sigma_trend = pm.HalfNormal('sigma_trend', sigma=0.1)  # trend innovation\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma_obs = pm.HalfNormal('sigma_obs', sigma=0.5)\n",
    "    \n",
    "    # Initial conditions\n",
    "    init_level = pm.Normal('init_level', mu=0, sigma=1)\n",
    "    init_trend = pm.Normal('init_trend', mu=0, sigma=0.1)\n",
    "    \n",
    "    # Trend process (random walk)\n",
    "    trend_init_dist = pm.Normal.dist(mu=init_trend, sigma=sigma_trend)\n",
    "    trend = pm.GaussianRandomWalk('trend',\n",
    "                                 mu=0,\n",
    "                                 sigma=sigma_trend,\n",
    "                                 init_dist=trend_init_dist,\n",
    "                                 steps=n_obs-1)\n",
    "    \n",
    "    # Level process (random walk + trend)\n",
    "    # This is more complex - we'll use a simpler approximation\n",
    "    level_innovation = pm.Normal('level_innovation', mu=0, sigma=sigma_level, shape=n_obs)\n",
    "    \n",
    "    # Construct level with trend\n",
    "    level = pm.Deterministic('level', \n",
    "                            pt.cumsum(pt.concatenate([[init_level], \n",
    "                                                     trend + level_innovation[1:]])))\n",
    "    \n",
    "    # Observations\n",
    "    obs = pm.Normal('obs', mu=level, sigma=sigma_obs, observed=births_standardized)\n",
    "\n",
    "# Sample from local trend model\n",
    "with local_trend_model:\n",
    "    trace_local_trend = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Local Linear Trend Model Results**:\")\n",
    "print(az.summary(trace_local_trend, var_names=['sigma_level', 'sigma_trend', 'sigma_obs']))\n",
    "\n",
    "# Analyze trend variability\n",
    "sigma_trend_mean = az.extract(trace_local_trend)['sigma_trend'].mean().item()\n",
    "print(f\"\\n📈 **Trend Dynamics Analysis**:\")\n",
    "print(f\"   • **Trend variability**: {sigma_trend_mean:.4f}\")\n",
    "if sigma_trend_mean > 0.05:\n",
    "    print(f\"   • **Interpretation**: Highly variable trend - changing growth rates\")\n",
    "else:\n",
    "    print(f\"   • **Interpretation**: Stable trend - consistent growth pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Stochastic Volatility Models\n",
    "\n",
    "**Stochastic volatility models** address the reality that uncertainty itself changes over time. These models are essential for financial time series where periods of calm alternate with periods of high volatility, but they're also valuable for any time series where the variability changes systematically.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "The standard stochastic volatility model consists of:\n",
    "\n",
    "**Returns equation**: $r_t = \\mu + \\sigma_t \\epsilon_t$\n",
    "\n",
    "**Volatility equation**: $\\log(\\sigma_t^2) = \\log(\\sigma_{t-1}^2) + \\nu_t$\n",
    "\n",
    "where $\\epsilon_t, \\nu_t \\sim \\mathcal{N}(0,1)$ are **independent innovations**.\n",
    "\n",
    "### Applications Beyond Finance\n",
    "\n",
    "While originally developed for financial markets, stochastic volatility models apply to many domains. **Environmental data** often shows changing variability due to seasonal effects or climate change. **Economic indicators** may have periods of stability and instability. **Biological processes** can exhibit changing variance due to external stressors or developmental stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with time-varying volatility for demonstration\n",
    "np.random.seed(42)\n",
    "n_periods = 200\n",
    "true_mu = 0.02\n",
    "true_tau = 0.1\n",
    "\n",
    "# Simulate log-volatility as random walk\n",
    "log_vol = np.cumsum(np.random.normal(0, true_tau, n_periods))\n",
    "vol = np.exp(log_vol / 2)\n",
    "\n",
    "# Simulate returns with time-varying volatility\n",
    "returns = np.random.normal(true_mu, vol)\n",
    "\n",
    "print(f\"📈 Generated {n_periods} synthetic returns with stochastic volatility\")\n",
    "print(f\"   Mean return: {returns.mean():.4f}\")\n",
    "print(f\"   Return volatility: {returns.std():.4f}\")\n",
    "print(f\"   Volatility range: [{vol.min():.3f}, {vol.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Volatility Model\n",
    "\n",
    "**Stochastic volatility models** capture **time-varying uncertainty** by allowing the variance to follow its own stochastic process. The standard specification is:\n",
    "\n",
    "**Returns equation**: $r_t = \\mu + \\sigma_t \\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "**Log-volatility equation**: $\\log(\\sigma_t) = \\log(\\sigma_{t-1}) + \\nu_t$ where $\\nu_t \\sim \\mathcal{N}(0, \\tau^2)$\n",
    "\n",
    "where:\n",
    "- $r_t$ are the **returns** (or observations)\n",
    "- $\\mu$ is the **mean return**\n",
    "- $\\sigma_t$ is the **time-varying volatility**\n",
    "- $\\tau^2$ controls the **volatility of volatility**\n",
    "- $\\epsilon_t, \\nu_t$ are **independent** innovations\n",
    "\n",
    "**Mathematical properties**:\n",
    "- **Log-normal volatility**: $\\sigma_t$ is always positive\n",
    "- **Volatility clustering**: High volatility periods tend to persist\n",
    "- **Heavy tails**: Returns distribution has fatter tails than normal\n",
    "- **Leverage effect**: Can be extended to include correlation between return and volatility shocks\n",
    "\n",
    "**Parameter interpretation**:\n",
    "- **Large $\\tau$**: Highly variable volatility (regime switching)\n",
    "- **Small $\\tau$**: Stable volatility (close to constant variance)\n",
    "- **Persistence**: $\\mathbb{E}[\\log(\\sigma_{t+1}) | \\log(\\sigma_t)] = \\log(\\sigma_t)$ (unit root in log-volatility)\n",
    "\n",
    "**Applications beyond finance**:\n",
    "- **Environmental data**: Changing measurement precision\n",
    "- **Economic indicators**: Periods of stability vs. uncertainty\n",
    "- **Biological processes**: Variable response to treatments\n",
    "- **Engineering systems**: Time-varying noise characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Volatility Model\n",
    "with pm.Model() as stoch_vol_model:\n",
    "    # Mean return parameter\n",
    "    mu = pm.Normal('mu', mu=0, sigma=0.1)\n",
    "    \n",
    "    # Volatility process parameters\n",
    "    tau = pm.HalfNormal('tau', sigma=0.2)  # Innovation in log-volatility\n",
    "    \n",
    "    # Log-volatility random walk\n",
    "    init_dist = pm.Normal.dist(mu=np.log(0.1), sigma=1)\n",
    "    log_sigma = pm.GaussianRandomWalk('log_sigma',\n",
    "                                     mu=0,\n",
    "                                     sigma=tau,\n",
    "                                     init_dist=init_dist,\n",
    "                                     steps=n_periods-1)\n",
    "    \n",
    "    # Convert to volatility (ensure positivity)\n",
    "    sigma = pm.Deterministic('sigma', pm.math.exp(log_sigma))\n",
    "    \n",
    "    # Likelihood (returns with time-varying volatility)\n",
    "    y_pred = pm.Normal('y_pred', mu=mu, sigma=sigma, observed=returns)\n",
    "\n",
    "# Sample from stochastic volatility model\n",
    "with stoch_vol_model:\n",
    "    trace_sv = pm.sample(1000, tune=2000, target_accept=0.9, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Stochastic Volatility Model Results**:\")\n",
    "print(az.summary(trace_sv, var_names=['mu', 'tau']))\n",
    "\n",
    "# Analyze volatility dynamics\n",
    "tau_mean = az.extract(trace_sv)['tau'].mean().item()\n",
    "sigma_posterior = az.extract(trace_sv)['sigma']\n",
    "vol_persistence = np.corrcoef(sigma_posterior[:-1], sigma_posterior[1:])[0,1]\n",
    "\n",
    "print(f\"\\n📈 **Volatility Analysis**:\")\n",
    "print(f\"   • **Volatility innovation**: {tau_mean:.4f}\")\n",
    "print(f\"   • **Volatility persistence**: {vol_persistence:.3f}\")\n",
    "print(f\"   • **Mean volatility**: {sigma_posterior.mean():.3f}\")\n",
    "if tau_mean > 0.1:\n",
    "    print(f\"   • **Interpretation**: Highly variable volatility - regime changes\")\n",
    "else:\n",
    "    print(f\"   • **Interpretation**: Stable volatility - gradual changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Gaussian Process Regression for Time Series\n",
    "\n",
    "**Gaussian Processes (GPs)** provide the ultimate flexibility in time series modeling by learning the temporal structure directly from data without imposing strong parametric assumptions. They excel when the functional form is unknown or when we want to capture complex, non-linear patterns.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Covariance functions** (kernels) define the relationship between points in time, encoding our assumptions about smoothness, periodicity, and other structural properties. **Length scales** control how quickly correlations decay with time separation. **Marginal variance** controls the overall variability of the process. **Hyperparameter learning** allows the model to adapt these properties to the data.\n",
    "\n",
    "### Advantages of GP Modeling\n",
    "\n",
    "**Non-parametric flexibility** means we don't need to specify a particular functional form in advance. **Uncertainty quantification** is built-in, providing credible intervals that naturally widen in regions with less data. **Kernel composition** allows combining different temporal patterns (trends, seasonality, noise) in principled ways. **Extrapolation** provides reasonable predictions beyond the training data with appropriate uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for GP regression (use subset for computational efficiency)\n",
    "n_gp = 100\n",
    "X_gp = np.arange(n_gp)[:, None]  # Time points (must be 2D for GP)\n",
    "y_gp = births_standardized[:n_gp]  # Observations\n",
    "\n",
    "print(f\"📊 GP data prepared: {n_gp} observations\")\n",
    "print(f\"   Time range: 0 to {n_gp-1}\")\n",
    "print(f\"   Data range: [{y_gp.min():.3f}, {y_gp.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process with RBF Kernel\n",
    "\n",
    "We'll start with a **Radial Basis Function (RBF)** kernel, also known as the squared exponential kernel. The RBF kernel is defined as:\n",
    "\n",
    "$$k(t_i, t_j) = \\sigma_f^2 \\exp\\left(-\\frac{(t_i - t_j)^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\sigma_f^2$ is the **signal variance** (marginal variance)\n",
    "- $\\ell$ is the **length scale** (correlation distance)\n",
    "- $t_i, t_j$ are **time points**\n",
    "\n",
    "**Mathematical properties**:\n",
    "- **Smoothness**: Infinitely differentiable (very smooth functions)\n",
    "- **Stationarity**: Covariance depends only on $|t_i - t_j|$\n",
    "- **Isotropy**: Same behavior in all directions\n",
    "- **Decay**: Correlation decreases exponentially with distance\n",
    "\n",
    "**Hyperparameter interpretation**:\n",
    "- **Large $\\ell$**: Long-range correlations, smooth trends\n",
    "- **Small $\\ell$**: Short-range correlations, rapid changes\n",
    "- **Large $\\sigma_f^2$**: High signal variance, large deviations from mean\n",
    "- **Small $\\sigma_f^2$**: Low signal variance, stays close to mean\n",
    "\n",
    "The complete GP model includes observation noise:\n",
    "$$y_t = f(t) + \\epsilon_t$$\n",
    "where $f(t) \\sim \\mathcal{GP}(0, k(t_i, t_j))$ and $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n",
    "\n",
    "This kernel assumes smooth, continuous functions and is excellent for capturing non-linear trends without sharp discontinuities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process with RBF Kernel\n",
    "with pm.Model() as gp_model:\n",
    "    # GP hyperparameters\n",
    "    length_scale = pm.HalfNormal('length_scale', sigma=10.0)  # temporal correlation\n",
    "    eta = pm.HalfNormal('eta', sigma=1.0)  # marginal standard deviation\n",
    "    \n",
    "    # Define the covariance function (RBF/Squared Exponential)\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(1, ls=length_scale)\n",
    "    \n",
    "    # GP prior\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma_gp = pm.HalfNormal('sigma_gp', sigma=0.5)\n",
    "    \n",
    "    # Observed data\n",
    "    y_pred = gp.marginal_likelihood('y_pred', X=X_gp, y=y_gp, sigma=sigma_gp)\n",
    "\n",
    "# Sample from the model\n",
    "with gp_model:\n",
    "    trace_gp = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)\n",
    "\n",
    "print(\"🎯 **Gaussian Process Model Results**:\")\n",
    "print(az.summary(trace_gp, var_names=['length_scale', 'eta', 'sigma_gp']))\n",
    "\n",
    "# Interpret GP hyperparameters\n",
    "length_scale_mean = az.extract(trace_gp)['length_scale'].mean().item()\n",
    "eta_mean = az.extract(trace_gp)['eta'].mean().item()\n",
    "sigma_gp_mean = az.extract(trace_gp)['sigma_gp'].mean().item()\n",
    "\n",
    "print(f\"\\n📈 **GP Analysis**:\")\n",
    "print(f\"   • **Length scale**: {length_scale_mean:.2f} time units\")\n",
    "print(f\"   • **Signal variance**: {eta_mean:.3f}\")\n",
    "print(f\"   • **Noise variance**: {sigma_gp_mean:.3f}\")\n",
    "print(f\"   • **Signal-to-noise ratio**: {eta_mean/sigma_gp_mean:.2f}\")\n",
    "if length_scale_mean > 20:\n",
    "    print(f\"   • **Interpretation**: Long-range correlations - smooth trends\")\n",
    "elif length_scale_mean > 5:\n",
    "    print(f\"   • **Interpretation**: Medium-range correlations - moderate smoothness\")\n",
    "else:\n",
    "    print(f\"   • **Interpretation**: Short-range correlations - rapid changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Selection\n",
    "\n",
    "With multiple models fitted to our data, we need principled methods for comparing their performance and selecting the most appropriate approach. **Bayesian model comparison** provides several tools for this purpose, each with different strengths and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models using WAIC (Widely Applicable Information Criterion)\n",
    "# Note: We'll compare models that use the same data\n",
    "models_dict = {\n",
    "    'Random Walk': trace_rw,\n",
    "    'RW with Drift': trace_rw_drift,\n",
    "    'AR(1)': trace_ar1,\n",
    "    'Linear Trend': trace_linear,\n",
    "    'Polynomial Trend': trace_poly,\n",
    "    'Seasonal Model': trace_seasonal,\n",
    "    'Local Level': trace_local_level\n",
    "}\n",
    "\n",
    "# Compute WAIC for model comparison\n",
    "comparison = az.compare(models_dict, ic='waic')\n",
    "print(\"🎯 **Model Comparison Results (WAIC)**:\")\n",
    "print(comparison)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot WAIC comparison\n",
    "az.plot_compare(comparison, ax=ax1)\n",
    "ax1.set_title('Model Comparison: WAIC Scores')\n",
    "\n",
    "# Plot model weights\n",
    "model_names = comparison.index\n",
    "weights = comparison['weight']\n",
    "ax2.bar(range(len(model_names)), weights, color='steelblue', alpha=0.7)\n",
    "ax2.set_xticks(range(len(model_names)))\n",
    "ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Model Weight')\n",
    "ax2.set_title('Model Weights (Probability)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret results\n",
    "best_model = comparison.index[0]\n",
    "best_weight = comparison.loc[best_model, 'weight']\n",
    "\n",
    "print(f\"\\n📊 **Model Selection Insights**:\")\n",
    "print(f\"   • **Best model**: {best_model} (weight: {best_weight:.3f})\")\n",
    "print(f\"   • **WAIC interpretation**: Lower values indicate better predictive performance\")\n",
    "print(f\"   • **Model weights**: Represent relative probability of each model\")\n",
    "if best_weight > 0.5:\n",
    "    print(f\"   • **Conclusion**: Strong evidence favors the {best_model} model\")\n",
    "else:\n",
    "    print(f\"   • **Conclusion**: Model uncertainty - consider ensemble approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "In this comprehensive section, we've explored the rich landscape of Bayesian time series models, from fundamental building blocks to sophisticated state-space and non-parametric approaches. Each model type offers unique advantages and is suited to different types of temporal patterns and modeling objectives.\n",
    "\n",
    "### Model Categories and Applications\n",
    "\n",
    "**Gaussian Random Walks** serve as the foundation for many time series models, providing smooth trend estimation and natural uncertainty propagation. These models excel when we need to capture gradual changes over time without imposing rigid functional forms. The ability to add drift terms makes them suitable for trending data, while their computational efficiency makes them practical for large datasets.\n",
    "\n",
    "**Bayesian Regression Models** offer maximum interpretability by explicitly modeling trend and seasonal components. The linear trend model provides a baseline for understanding directional movement, while polynomial extensions capture more complex curvature patterns. Fourier-based seasonal models elegantly handle periodic patterns and can accommodate multiple seasonal frequencies simultaneously.\n",
    "\n",
    "**State-Space Models** represent the most flexible framework for separating latent dynamics from observation processes. The local level model excels at signal extraction from noisy data, while local linear trend models capture time-varying growth rates. These models naturally handle missing data and provide a foundation for more complex hierarchical structures.\n",
    "\n",
    "**Stochastic Volatility Models** address the crucial reality that uncertainty itself changes over time. While originally developed for financial applications, these models apply broadly to any domain where variability patterns are of interest. They provide insights into regime changes and help identify periods of stability versus instability.\n",
    "\n",
    "**Gaussian Process Models** offer ultimate flexibility by learning temporal structure directly from data. Their non-parametric nature makes them ideal for exploratory analysis and situations where the functional form is unknown. The ability to compose kernels allows for sophisticated modeling of multiple temporal patterns simultaneously.\n",
    "\n",
    "### Practical Modeling Guidelines\n",
    "\n",
    "**Start simple** and build complexity gradually. Random walks and linear trends provide excellent baselines that often perform surprisingly well. **Consider the data characteristics** when choosing models—trending data benefits from drift terms or explicit trend modeling, while seasonal data requires appropriate periodic components.\n",
    "\n",
    "**Use domain knowledge** to guide model selection and prior specification. Understanding the underlying process generating your data helps choose appropriate model structures and realistic parameter ranges. **Validate thoroughly** using posterior predictive checks and out-of-sample evaluation to ensure models capture essential patterns without overfitting.\n",
    "\n",
    "**Embrace uncertainty** in model selection. When multiple models perform similarly, consider ensemble approaches that average across models weighted by their posterior probabilities. This approach often provides more robust predictions than relying on a single \"best\" model.\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "**Model complexity** directly impacts computational requirements. Simple models like random walks and linear regression scale well to large datasets, while Gaussian processes become computationally intensive for more than a few hundred observations. **Prior specification** significantly affects convergence—weakly informative priors often provide the best balance between regularization and flexibility.\n",
    "\n",
    "**Convergence diagnostics** remain crucial regardless of model complexity. Always examine R-hat values, effective sample sizes, and trace plots to ensure reliable inference. **Computational efficiency** can often be improved through data standardization, appropriate parameterizations, and careful prior choices.\n",
    "\n",
    "**Next**: In the following sections, we'll explore model evaluation techniques, forecasting methods, and practical applications that build upon these foundational modeling approaches.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Bayesian time series modeling offers a rich toolkit for capturing temporal patterns while quantifying uncertainty\n",
    "- Random walks provide flexible trend modeling, regression approaches offer interpretable component decomposition\n",
    "- State-space models separate signal from noise, stochastic volatility captures changing uncertainty\n",
    "- Gaussian processes provide non-parametric flexibility for complex temporal patterns\n",
    "- Model selection should balance complexity with interpretability, always validated through rigorous posterior checking and out-of-sample evaluation\n",
    "- The Bayesian framework naturally handles uncertainty in both parameters and model structure, providing robust foundations for forecasting and decision-making under uncertainty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
