{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction and Fundamentals\n",
    "\n",
    "#### PyData London 2025 - Bayesian Time Series Analysis with PyMC\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation for Bayesian Time Series Analysis\n",
    "\n",
    "Traditional time series analysis has served us well for decades, but it suffers from a fundamental limitation: **uncertainty is often treated as an afterthought**. When we build an ARIMA model or apply exponential smoothing, we typically get point forecasts with some measure of \"confidence intervals\" that are often based on strong distributional assumptions and may not reflect the true uncertainty in our predictions.\n",
    "\n",
    "**Bayesian time series analysis fundamentally changes this perspective** by treating uncertainty as a first-class citizen throughout the entire modeling process. Instead of obtaining single-valued predictions, we get complete **probability distributions** over all possible future values, conditioned on our data and our modeling assumptions.\n",
    "\n",
    "### Handling Uncertainty\n",
    "\n",
    "Consider predicting next month's sales. A classical approach might tell us \"we expect 1,000 units Â± 100.\" But what does this really mean? Are we 90% confident? 95%? What's the shape of this uncertaintyâ€”is it symmetric or skewed?\n",
    "\n",
    "A Bayesian approach instead tells us: \"There's a 20% chance sales will be below 950 units, a 50% chance they'll be between 950 and 1,050, and a 30% chance they'll exceed 1,050.\" This **probabilistic language** is much more natural for decision-making under uncertainty.\n",
    "\n",
    "### Incorporating Prior Knowledge\n",
    "\n",
    "Time series data often come with rich domain knowledge. We might know that:\n",
    "- Sales are typically higher in December due to holiday shopping\n",
    "- Stock volatility tends to cluster (high volatility periods are followed by more high volatility)\n",
    "- Economic indicators don't change dramatically from day to day\n",
    "\n",
    "**Bayesian methods provide a principled way to incorporate this knowledge** through prior distributions, rather than treating each dataset as if we know nothing about the world.\n",
    "\n",
    "### Flexibility for Complex Patterns\n",
    "\n",
    "Bayesian approaches allow us to naturally handle:\n",
    "- **Missing data**: Rather than imputing or interpolating, we can model missingness directly\n",
    "- **Irregular spacing**: No need to artificially regularize timestamps\n",
    "- **Multiple seasonality**: Hierarchical models naturally accommodate daily, weekly, and yearly patterns\n",
    "- **Changing relationships**: Time-varying parameters are natural in the Bayesian framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Libraries loaded successfully!\n",
      "PyMC version: 5.22.0\n",
      "ArviZ version: 0.21.0\n",
      "NumPy version: 2.2.5\n",
      "Polars version: 1.27.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Section 1\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import warnings\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"ğŸ“Š Libraries loaded successfully!\")\n",
    "print(\"PyMC version:\", pm.__version__)\n",
    "print(\"ArviZ version:\", az.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Polars version:\", pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Characteristics of Time Series Data\n",
    "\n",
    "Before diving into Bayesian methods, let's establish a solid foundation in time series fundamentals. Understanding these characteristics is crucial for building appropriate models.\n",
    "\n",
    "### Trend, Seasonality, and Noise Components\n",
    "\n",
    "Most time series can be decomposed into several interpretable components:\n",
    "\n",
    "1. **Trend ($T_t$)**: The long-term direction of the data\n",
    "2. **Seasonality ($S_t$)**: Regular, predictable patterns that repeat over fixed periods\n",
    "3. **Cyclical patterns ($C_t$)**: Longer-term fluctuations without fixed periods\n",
    "4. **Irregular/Noise ($\\epsilon_t$)**: Random fluctuations that cannot be explained by the other components\n",
    "\n",
    "We can write this **decomposition** as either:\n",
    "\n",
    "**Additive**: $y_t = T_t + S_t + C_t + \\epsilon_t$\n",
    "\n",
    "**Multiplicative**: $y_t = T_t \\times S_t \\times C_t \\times \\epsilon_t$\n",
    "\n",
    "### Stationarity and Autocorrelation\n",
    "\n",
    "The defining characteristic of time series data is **temporal dependence**â€”observations that are close in time are typically more similar than observations that are far apart. This violates the independence assumption that underlies most statistical methods, requiring specialized approaches.\n",
    "\n",
    "**Mathematically**, we can express this as:\n",
    "\n",
    "$$\\text{Corr}(y_t, y_{t+h}) \\neq 0$$\n",
    "\n",
    "for some lag $h > 0$. This **autocorrelation** is what makes time series prediction possibleâ€”if future values were completely unrelated to past values, prediction would be impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Births Dataset Overview:\n",
      "   â€¢ Total months: 228\n",
      "   â€¢ Date range: 1970 to 1988\n",
      "   â€¢ Monthly births range: 237,302 to 354,599\n",
      "   â€¢ Average monthly births: 293389\n",
      "   â€¢ Standard deviation: 25082\n",
      "\n",
      "ğŸ“‹ First few observations:\n",
      "shape: (10, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ year â”† month â”† births â”‚\n",
      "â”‚ ---  â”† ---   â”† ---    â”‚\n",
      "â”‚ i64  â”† i64   â”† i64    â”‚\n",
      "â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 1970 â”† 1     â”† 302278 â”‚\n",
      "â”‚ 1970 â”† 2     â”† 281488 â”‚\n",
      "â”‚ 1970 â”† 3     â”† 307448 â”‚\n",
      "â”‚ 1970 â”† 4     â”† 287090 â”‚\n",
      "â”‚ 1970 â”† 5     â”† 298140 â”‚\n",
      "â”‚ 1970 â”† 6     â”† 303378 â”‚\n",
      "â”‚ 1970 â”† 7     â”† 330452 â”‚\n",
      "â”‚ 1970 â”† 8     â”† 331326 â”‚\n",
      "â”‚ 1970 â”† 9     â”† 332496 â”‚\n",
      "â”‚ 1970 â”† 10    â”† 324422 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the births dataset - a classic time series example\n",
    "# Handle null values in the data\n",
    "births_data = pl.read_csv('../data/births.csv', null_values=['null', 'NA', '', 'NULL'])\n",
    "\n",
    "# Filter out rows with null days if any exist\n",
    "births_data = births_data.filter(pl.col('day').is_not_null())\n",
    "\n",
    "# Aggregate to monthly data\n",
    "monthly_births = (births_data\n",
    "    .group_by(['year', 'month'])\n",
    "    .agg(pl.col('births').sum())\n",
    "    .sort(['year', 'month'])\n",
    ")\n",
    "\n",
    "# Create valid dates using the first day of each month\n",
    "monthly_births = monthly_births.with_columns([\n",
    "    pl.date(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "])\n",
    "\n",
    "# Focus on a 20-year period for clarity (1970-1990)\n",
    "births_subset = (monthly_births\n",
    "    .filter((pl.col('year') >= 1970) & (pl.col('year') <= 1990))\n",
    "    .with_row_index('index')\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ˆ Births Dataset Overview:\")\n",
    "print(f\"   â€¢ Total months: {births_subset.height}\")\n",
    "print(f\"   â€¢ Date range: {births_subset['year'].min()} to {births_subset['year'].max()}\")\n",
    "print(f\"   â€¢ Monthly births range: {births_subset['births'].min():,} to {births_subset['births'].max():,}\")\n",
    "print(f\"   â€¢ Average monthly births: {births_subset['births'].mean():.0f}\")\n",
    "print(f\"   â€¢ Standard deviation: {births_subset['births'].std():.0f}\")\n",
    "\n",
    "# Display the first few observations\n",
    "print(f\"\\nğŸ“‹ First few observations:\")\n",
    "print(births_subset.select(['year', 'month', 'births']).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Techniques\n",
    "\n",
    "Before building Bayesian models, proper data preprocessing is essential. This section demonstrates key preprocessing techniques that prepare time series data for effective modeling.\n",
    "\n",
    "### Why Preprocessing Matters\n",
    "\n",
    "Time series preprocessing serves several critical purposes:\n",
    "\n",
    "1. **Numerical Stability**: Standardization helps MCMC samplers converge more reliably\n",
    "2. **Prior Specification**: Normalized data makes it easier to specify reasonable priors\n",
    "3. **Model Interpretation**: Standardized coefficients are easier to interpret and compare\n",
    "4. **Computational Efficiency**: Well-scaled data leads to faster sampling\n",
    "\n",
    "### Common Preprocessing Techniques\n",
    "\n",
    "1. **Standardization**: $(x - \\mu) / \\sigma$ - Centers data at 0 with unit variance\n",
    "2. **Min-Max Normalization**: $(x - \\min) / (\\max - \\min)$ - Scales to [0,1] range\n",
    "3. **Log Transformation**: $\\log(x)$ - Stabilizes variance and handles exponential growth\n",
    "4. **Differencing**: $x_t - x_{t-1}$ - Removes trends and induces stationarity\n",
    "5. **Seasonal Decomposition**: Separates trend, seasonal, and residual components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Preprocessing Results:\n",
      "   â€¢ Original data: mean=293389, std=25027\n",
      "   â€¢ Standardized: mean=-0.000, std=1.000\n",
      "   â€¢ Min-Max norm: min=0.000, max=1.000\n",
      "   â€¢ Log transform: mean=12.586, std=0.086\n",
      "\n",
      "âœ… **Selected preprocessing**: Standardized data (mean=0, std=1)\n",
      "   This choice provides:\n",
      "   â€¢ Numerical stability for MCMC sampling\n",
      "   â€¢ Easy interpretation of parameters\n",
      "   â€¢ Natural scale for prior specification\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate different preprocessing transformations\n",
    "original_data = births_subset['births'].to_numpy()\n",
    "\n",
    "# 1. Standardization (most common for Bayesian modeling)\n",
    "standardized = (original_data - original_data.mean()) / original_data.std()\n",
    "\n",
    "# 2. Min-Max normalization  \n",
    "min_max_norm = (original_data - original_data.min()) / (original_data.max() - original_data.min())\n",
    "\n",
    "# 3. Log transformation\n",
    "log_transform = np.log(original_data)\n",
    "\n",
    "print(\"ğŸ“Š Preprocessing Results:\")\n",
    "print(f\"   â€¢ Original data: mean={original_data.mean():.0f}, std={original_data.std():.0f}\")\n",
    "print(f\"   â€¢ Standardized: mean={standardized.mean():.3f}, std={standardized.std():.3f}\")\n",
    "print(f\"   â€¢ Min-Max norm: min={min_max_norm.min():.3f}, max={min_max_norm.max():.3f}\")\n",
    "print(f\"   â€¢ Log transform: mean={log_transform.mean():.3f}, std={log_transform.std():.3f}\")\n",
    "\n",
    "# Choose standardized data for modeling (most common choice)\n",
    "births_standardized = standardized\n",
    "print(f\"\\nâœ… **Selected preprocessing**: Standardized data (mean=0, std=1)\")\n",
    "print(f\"   This choice provides:\")\n",
    "print(f\"   â€¢ Numerical stability for MCMC sampling\")\n",
    "print(f\"   â€¢ Easy interpretation of parameters\")\n",
    "print(f\"   â€¢ Natural scale for prior specification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we've covered:\n",
    "\n",
    "1. **Motivation for Bayesian time series analysis**: Uncertainty quantification, prior knowledge incorporation, and flexibility\n",
    "2. **Key time series characteristics**: Trend, seasonality, autocorrelation, and stationarity\n",
    "3. **Essential preprocessing techniques**: Standardization, normalization, and transformations\n",
    "\n",
    "**Next**: In Section 2, we'll dive into the fundamentals of Bayesian inference and learn how to use PyMC for time series modeling.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Bayesian methods treat uncertainty as a first-class citizen\n",
    "- Time series data have unique characteristics that require specialized modeling approaches\n",
    "- Proper preprocessing is essential for successful Bayesian time series modeling\n",
    "- Standardization is typically the best preprocessing choice for Bayesian models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
